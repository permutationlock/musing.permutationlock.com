---
.title = "Linux Kernel Development",
.date = @date("2025-10-20T00:00:00"),
.author = "Aven",
.layout = "post.shtml",
.draft = false,
.custom = {},
---

I have been a Linux user space programmer for well over ten years, but only
recently dipped my toe into kernel development.
In this article I'll go over Linux system emulation, the basics of
the Linux kernel build system, the structure of a kernel module C project,
the key kernel module utilities (`insmod`, `rmmod`, `modprobe`),
and debugging code that runs in kernel space with `gdb`.

## Requirements

I will assume that we are working on a modern Linux system.
We will first need to install the [Linux kernel build dependencies][35].
On almost all Linux distributions, these will be available through
the package manager. To install the development version of a
library (i.e. to install header files and [`pkg-config`][36] files in addition
to the library itself), the development package will often be required.

For system emulation we will need the
`qemu-system-xxx` emulator for our target architecture .
To take advantage
of [KVM][2] (and avoid having to set up a cross-compilation toolchain),
the emulated system architecture should match our
physical hardware; e.g. if our host system is running on an
`x86_64` CPU, then we should install and use `qemu-system-x86_64`.

Finally, to debug kernel space code, we will need the GNU debugger ([`gdb`][7]).

## Building the Linux kernel

The Linux kernel uses a `make`-based build system called `Kbuild` and
a custom `Kconfig` configuration system. We'll delve a little into
how both of these work in the kernel module section, but for now
we just need to clone the kernel source and use `make` to configure and
build the kernel.

```bash
$ mkdir linux_kvm_debug
$ cd linux_kvm_debug
$ wget https://github.com/torvalds/linux/archive/refs/tags/v6.17.tar.gz
$ tar -xvf v6.17.tar.gz
```

We'll use the following base config file with the
options required for `qemu` KVM emulation and GDB kernel debugging.

```bash
# mini.config

# minimum for `qemu` toybox initramfs boot
CONFIG_BLK_DEV_INITRD=y
CONFIG_INITRAMFS_SOURCE=""
CONFIG_STANDALONE=y
CONFIG_PREVENT_FIRMWARE_BUILD=y
CONFIG_SYSCTL=y
CONFIG_UNIX98_PTYS=y
CONFIG_SWAP=y
CONFIG_BINFMT_SCRIPT=y
CONFIG_DEVTMPFS=y
CONFIG_DEVTMPFS_MOUNT=y
CONFIG_BLK_DEV_WRITE_MOUNTED=y

# networking
CONFIG_ETHERNET=y
CONFIG_UNIX=y
CONFIG_IPV6=y
CONFIG_FORCEDETH=y
CONFIG_E1000=y
CONFIG_E1000E=y

# add ext4 filesystem to mount volumes
CONFIG_EXT4=y

# enable modules
CONFIG_MODULES=y
CONFIG_MODULE_UNLOAD=y

# allow kernel GDB debugging
CONFIG_DEBUG_INFO=y
CONFIG_DEBUG_INFO_COMPRESSED_NONE=y
CONFIG_DEBUG_INFO_DWARF_TOOLCHAIN_DEFAULT=y
CONFIG_GDB_SCRIPTS=y
CONFIG_FRAME_POINTER=y
CONFIG_RANDOMIZE_BASE=n # optional, may use `nokaslr` parameter instead

# KVM guest (from `kernel/configs/kvm_guest.config`)
CONFIG_NET=y
CONFIG_NET_CORE=y
CONFIG_NETDEVICES=y
CONFIG_BLOCK=y
CONFIG_BLK_DEV=y
CONFIG_NETWORK_FILESYSTEMS=y
CONFIG_INET=y
CONFIG_TTY=y
CONFIG_SERIAL_8250=y
CONFIG_SERIAL_8250_CONSOLE=y
CONFIG_IP_PNP=y
CONFIG_IP_PNP_DHCP=y
CONFIG_BINFMT_ELF=y
CONFIG_PCI=y
CONFIG_PCI_MSI=y
CONFIG_DEBUG_KERNEL=y
CONFIG_VIRTUALIZATION=y
CONFIG_HYPERVISOR_GUEST=y
CONFIG_PARAVIRT=y
CONFIG_KVM_GUEST=y
CONFIG_S390_GUEST=y
CONFIG_VIRTIO=y
CONFIG_VIRTIO_MENU=y
CONFIG_VIRTIO_PCI=y
CONFIG_VIRTIO_BLK=y
CONFIG_VIRTIO_CONSOLE=y
CONFIG_VIRTIO_NET=y
CONFIG_9P_FS=y
CONFIG_NET_9P=y
CONFIG_NET_9P_VIRTIO=y
CONFIG_SCSI_LOWLEVEL=y
CONFIG_SCSI_VIRTIO=y
CONFIG_VIRTIO_INPUT=y
CONFIG_DRM_VIRTIO_GPU=y
```

The `allnoconfig` command will use the `KCONFIG_ALLCONFIG` file as a base,
set basic architecture defaults, and disable all other config options.

```bash
$ mkdir linux-6.17_build
$ make -C linux-6.17 O=$PWD/linux-6.17_build \
>    ARCH=x86_64 \
>    KCONFIG_ALLCONFIG=$PWD/mini.config \
>    allnoconfig
$ make -C linux-6.17 O=$PWD/linux-6.17_build -j$(nproc)
```

The compiled kernel image will be located in
`linux-6.17_build/arch/x86/boot/bzImage`. If you are using a different architecture,
the process should be the same but with `x86`/`x86_64` swapped
for your host architecture.

## Creating an initial RAM filesystem

The primary way to boot a Linux system is to use an inital RAM
filesystem (initramfs). An initramfs is simply a directory that
has been packaged into a [`cpio`][19] archive (and optionally compressed).
On boot the initramfs is mounted at `/` and then `/init` is executed
as the initial user space process.

**Note:** An initramfs can be built into the kernel binary itself, or be provided
separately. We will need to boot from several different initramfs
archives to test our debug Kernel, so we'll specify the initramfs archive
to use on each boot.

For our first initramfs we'll create a C program that prints
`"Hello, World!"`. Note that if we use the host C compiler with its default flags,
then it will compile a binary targeting the C "runtime" for the host system. I.e.
it will assume that the host lib directories (`/lib`, `/usr/lib`, etc.) are
present in the initial RAM filesystem.

We could copy our host sysroot directly into the initramfs, but an easier
option is to use the `nolibc` static C runtime provided by the Linux kernel.
The kernel build system exposes a `make` command to produce a `nolibc`
build sysroot.

```bash
$ mkdir nolibc
$ make -C linux-6.17/tools/include/nolibc \
>    ARCH=x86_64 OUTPUT=nolibc/ headers_standalone
```

By default, the Linux kernel will mount the inital RAM filesystem as `/`
and look for a `/init` executable to run as the first user-space
process. We will write a basic `init.c` file and compile a statically linked
`init` executable.

```c
/* init_c/init.c */

/* crt.h: exports `_start` symbol to init runtime and call `main` */
#include <crt.h>
/* sys.h: defines `reboot` function */
#include <sys.h>
/* unistd.h: defines `sleep`, `read`, and `write` functions */
#include <unistd.h>

int main(void) {
    /* try to wait for kernel boot messages to finish */
    sleep(1);

    /* write our message to standard output */
    const char msg[] = "Hello, World!\n";
    write(STDOUT_FILENO, msg, sizeof(msg));

    /* wait for user to press enter by reading from standard input */
    char buff[1];
    read(STDIN_FILENO, buff, sizeof(buff));

    /* reboot the machine */
    reboot(LINUX_REBOOT_CMD_RESTART);
}
```

```bash
$ gcc -static -fno-stack-protector -nostdlib -nostdinc \
>    -I nolibc/sysroot/include \
>    -o init_c/init \
>    init_c/init.c
```

The `init` static executable will wait one second, write `"Hello, World!"`, and
then restart the computer after the user presses enter. Next we'll use `cpio`
to package the `init_c` directory into an initramfs `newc` archive.

```bash
$ cd init_c
$ find . | cpio -o --format=newc > ../init_c.cpio
$ cd ..
```

With a kernel and an inital RAM filesystem containing an init executable, we can boot
an emulated system using `qemu`.

```bash
$ qemu-system-x86_64 --enable-kvm -nographic -no-reboot \
>    -kernel linux-6.17_build/arch/x86/boot/bzImage \
>    -initrd init_c.cpio \
>    -append "HOST=x86_64 console=ttyS0"
(vm) Linux version 6.17.0 <SYSTEM SPECIFIC STUFF>
(vm) Command line: HOST=x86_64 console=ttyS0
(vm) # ...
(vm) Run /init as init process
(vm) Hello, World!
```

You should see a string of kernel boot messages, followed by `Hello, World!`.
Pressing the `Enter` key should kill the virtual machine.

## Debugging the Kernel with GDB

In this section we will attach a `gdb` session to the kernel process
running in our `qemu` virtual machine. Our kernel was
built with debug information and `gdb` python scripts enabled (see
`kvm_debug_guest.config`), so all we need to do before we can open
`gdb` is add a few `qemu` flags.

```bash
$ qemu-system-x86_64 --enable-kvm -nographic -no-reboot -s -S \
>    -kernel linux-6.17_build/arch/x86/boot/bzImage \
>    -initrd init_c.cpio \
>    -append "HOST=x86_64 console=ttyS0"
```

The `-s` flag (short for `-gdb tcp::1234`) hosts a `gdb` server at address `:1234`
attached to the kernel process. The `-S` flag halts the CPU so the
kernel will not start booting before our `gdb` session can connect to the server.

We can now connect a `gdb` session and begin debugging.

```bash
$ gdb -q linux-6.17_build/vmlinux
Reading symbols from /home/aven/linux_kvm_debug/linux-6.17_build/vmlinux...
(gdb) target remote :1234
Remote debugging using :1234
0x000000000000fff0 in ?? ()
(gdb) hbreak start_kernel
Hardware assisted breakpoint 1 at 0xffffffff81a6d630: file /home/aven/docs/projects/vms/kvm_dbg/linux-6.17/init/main.c, line 899.
(gdb) c
Continuing.

Breakpoint 1, start_kernel ()
    at /home/aven/docs/projects/vms/kvm_dbg/linux-6.17/init/main.c:899
899		char *command_line;
(gdb)
```

Above we open the uncompressed kernel binary `vmlinux` with `gdb`, connect to
the `qemu` `gdbserver` target at address `:1234`, place a hardware
breakpoint at `start_kernel`, and unpause the CPU until the breakpoint
is hit. From here we can step through the kernel startup process as we we please.

However, we know that our `init` binary will eventually be run
and issue a `write` system call. We can search the kernel for
the function name that will be called for `write`.

```bash
$ grep -A4 -rn "SYSCALL_DEFINE.\?(write," linux-6.17/
fs/read_write.c:746:SYSCALL_DEFINE3(write, unsigned int, fd, const char __user *, buf,
fs/read_write.c-747-        size_t, count)
fs/read_write.c-748-{
fs/read_write.c-749-    return ksys_write(fd, buf, count);
fs/read_write.c-750-}
```

So we could either break on the `ksys_write` symbol (`hbreak ksys_write`) or use the source location
to set a breakpoint directly in the syscall (`hbreak fs/read_write.c:749`). The result I see continuing
from the `start_kernel` breakpoint we hit above is shown below.

```bash
(gdb) hbreak fs/read_write.c:749
Hardware assisted breakpoint 3 at 0xffffffff814156a4: file /home/aven/linux_kvm_debug/linux-6.17/fs/read_write.c, line 749.
(gdb) c
Continuing.

Breakpoint 2, __do_sys_write (fd=1, buf=0x7ffde9ee7c51 "Hello, World!\n", count=15)
    at /home/aven/docs/projects/vms/kvm_dbg/linux-6.17/fs/read_write.c:749
749		return ksys_write(fd, buf, count);
(gdb)
```

We interrupted the kernel side of the `"Hello, World!"` write system call
from our `init` process!

## Adding a shell to our initramfs with `toybox`

Our current initramfs contains a single `init` binary we compiled using
`nolibc`. While such single binary systems can be useful for testing
or building bespoke devices, in general we expect Linux systems to provide
a Unix style shell with a standard set of command line utilities.
Most GNU+Linux distributions
use the [GNU core utilites][20], but these are a somewhat heavy dependency
and provide far more than we need for a simple debug system.

There are several Unix-in-a-box projects that provide a single
"swiss army knife binary" containing all of the basic command-line utilites,
the most popular being [`busybox`][11] and
[`toybox`][12]. We'll be using `toybox` for our initramfs in this section, but later
we'll build an Alpine Linux system based on `busybox`.

```bash
$ wget https://landley.net/toybox/downloads/binaries/latest/toybox-x86_64
$ chmod +x toybox-x86_64
$ mkdir toybox/ccc
$ mkdir -p init_toybox/dev
$ mkdir -p init_toybox/proc
$ mkdir -p init_toybox/sys
$ mkdir -p init_toybox/bin
$ mkdir -p init_toybox/home
$ mv toybox-x86_64 init_toybox/bin/toybox
$ cd init_toybox/bin
$ for i in $(./toybox); do ln -s toybox $i; done
$ cd ../..
```

**Note:** If you are uncomfortable downloading the `toybox` binary,
you may also build it from source. You will need to use a C toolchain
capable of building static `x86_64` binaries, e.g. a `gcc` or `clang`
toolchain based on [`musl` libc][36].

We now have an `init_toybox` sysroot containing a `bin` directory
with a symlink file for each of the `toybox` commands. The `toybox`
binary checks the filename it was executed as to determine the command
to execute, and running the `toybox` binary itself without any arguments
prints a list of command names. E.g. the `for` loop above created
the symlink `bin/sh` to `bin/toybox`, and running `./bin/sh` is
equivalent to executing `./bin/toybox sh` which starts a shell.

Instead of compiling a static `init` ELF binary from C, we will write
a shell script to be executed with `bin/sh`. The init script below is
inspired by the init script generated by `toybox/mkroot/mkroot.sh`.

**Note:** When a [shebang][18] (`#!`) appears at the start of a text file,
the file may be used as if it were a binary executable. The path following the shebang
is executed with the path to the original text file passed as an argument.

```bash
#!/bin/sh
# init_toybox/init
export HOME=/home PATH=/bin
mount -t proc proc proc
mount -t sysfs sys sys
mount -t devtmpfs dev dev
ln -sf /proc/self/fd/0 /dev/stdin
ln -sf /proc/self/fd/1 /dev/stdout
ln -sf /proc/self/fd/2 /dev/stderr
mkdir -p dev/shm && chmod +t /dev/shm
mkdir -p dev/pts && mount -t devpts dev/pts dev/pts
sleep 1
echo "Welcome to sh!"
setsid -c /bin/sh <>/dev/ttyS0 >&0 2>&1
reboot -f
```

Our `init` script mounts the special [`/proc`][14],
[`/sys`][15], [`/dev`][13], [`/dev/pts`][16], and [`/dev/shm`][17] Linux
filesystems, sleeps for one second, prints a welcome message,
and starts a new session running `/bin/sh`. When
the `/bin/sh` session exits, the system is rebooted.

The `setsid` line is confusing to parse if you aren't
familiar with the arcane semantics of Unix shells, so lets break it
down piece-by-piece.

 1. The command `setsid -c /bin/sh` starts a new
   session with the same controlling `tty` as the current session,
   running `/bin/sh` as its initial process.
 2. The `n<>file` command opens `file` for reading and writing on file descriptor `n`. If
   no `n` is specified, then it defaults to descriptor `0`. Thus 
   `<>/dev/tty0` opens `/dev/tty0` for reading and writing on file descriptor `0`.
 3. The `n>&m` command makes file descriptor `n` a copy of the output
   file descriptor `m`. If `n` is not specified, then it defaults to descriptor `1`.
   Thus `>&0` makes descriptor `1` an output copy of descriptor `0`,
   and `2>&1` makes descriptor `2` an output copy of descriptor `1`.

Taken together, the `setsid` line creates a new session
with the same controlling `tty` as `init` that runs `/bin/sh` in its
inital process with `/dev/tty0` open as input on
file descriptor `0` and output on descriptors `1` and `2`. In other words,
it just starts a shell session with the stdin, stdout,
and stderr file descriptors!

**Note:** We
specified the initial active `tty` to be `ttyS0` in our `qemu`
parameters, but that probably shouldn't be hard-coded into
the script. The active `tty` should instead be parsed from
`/sys/class/tty/console/active`, e.g. using `sed` as shown below.

```bash
setsid -c /bin/sh <>/dev/$(sed '$s@.*[ /]@@' /sys/class/tty/console/active) >&0 2>&1
```

We may now package the `init_toybox` directory into an initramfs and boot the kernel
to an `sh` shell.

```bash
$ chmod +x init_toybox/init
$ cd init_toybox
$ find . | cpio -o --format=newc > ../init_toybox.cpio
$ cd ..
$ qemu-system-x86_64 --enable-kvm -nographic -no-reboot -s \
>    -kernel linux-6.17_build/arch/x86/boot/bzImage \
>    -initrd init_toybox.cpio \
>    -append "HOST=x86_64 console=ttyS0"
(vm) Linux version 6.17.0 <SYSTEM SPECIFIC STUFF>
(vm) Command line: HOST=x86_64 console=ttyS0
(vm) # ...
(vm) Run /init as init process
(vm) Welcome to sh!
(vm) $ 
```

## Building, loading, and debugging kernel modules

The Linux kernel allows code to be dynamically loaded at runtime through
modules. In that sense, modules are a loose kernel space equivalent
to user space shared libraries.

Kernel modules are generally built using the `Kbuild` makefiles from
the source tree of the Linux kernel being targeted. In this
section we'll write a simple `"Hello, Kernel!"` module that prints a
message to the kernel log ring buffer.

```c
/* hello_mod/hello.c */

#include <linux/module.h>
#include <linux/printk.h>

static int __init hello_init(void) {
    pr_info("Hello, Kernel!\n");
    return 0;
}

static void __exit hello_exit(void) {
    pr_info("Bye, Kernel :(\n");
}

module_init(hello_init);
module_exit(hello_exit);

MODULE_LICENSE("GPL");
MODULE_AUTHOR("Aven Bross <email@example.com>");
MODULE_DESCRIPTION("A kernel space hello world");
MODULE_VERSION("0.1");
```

The `hello.c` constitutes a tiny Kernel module that does nothing but
log when it is loaded and unloaded. We can build it with `make` using
the following Kbuild file.

```bash
# hello_mod/Kbuild
obj-m := hello.o
```

```bash
$ make -C linux-6.17_build M=$PWD/hello_mod
```

Now we should have a `hello.ko` module object in the source directory.

```bash
$ modinfo hello_mod/hello.ko
filename:       /home/aven/docs/projects/vms/kvm_dbg/hello_mod/hello.ko
version:        0.1
description:    A kernel space hello world
author:         Aven Bross <email@example.com>
license:        GPL
srcversion:     42A234CAB9C4A0A5982807E
depends:        
name:           hello
vermagic:       6.17.0 mod_unload
```

A simple way to have the module available on our
virtual machine is to package it directly into the initramfs.

```bash
$ cp hello_mod/hello.ko init_toybox/home/
$ cd init_toybox
$ find . | cpio -o --format=newc > ../init_toybox_hello.cpio
$ cd ..
$ qemu-system-x86_64 --enable-kvm -nographic -no-reboot -s \
>    -kernel linux-6.17_build/arch/x86/boot/bzImage \
>    -initrd init_toybox_hello.cpio \
>    -append "HOST=x86_64 console=ttyS0"
(vm) Linux version 6.17.0 <SYSTEM SPECIFIC STUFF>
(vm) Command line: HOST=x86_64 console=ttyS0
(vm) # ...
(vm) Run /init as init process
(vm) Welcome to sh!
(vm) $ insmod home/hello.ko
(vm) hello: loading out-of-tree module taints kernel.
(vm) Hello, Kernel!
(vm) $ lsmod
(vm) Module                  Size  Used by
(vm) hello                  12288  0
(vm) $ rmmod hello
(vm) Bye, Kernel :(
(vm) $ lsmod
(vm) Module                  Size  Used by
(vm) $ dmesg
(vm) [    0.000000] Linux version 6.17.0 <VERSION SPECIFIC STUFF>
(vm) [    0.000000] Command line: HOST=x86_64 console=ttyS0
(vm) # ...
(vm) [    0.183990] Run /init as init process
(vm) [    0.184379] with arguments:
(vm) [    0.184381] /init
(vm) [    0.184382] with environment:
(vm) [    0.184383] HOME=/
(vm) [    0.184384] TERM=linux
(vm) [    0.184384] HOST=x86_64
(vm) [    7.758562] hello: loading out-of-tree module taints kernel.
(vm) [    7.760936] Hello, Kernel!
(vm) [   65.639464] Bye, Kernel :(
```

If we want to be able to run `make` in the `hello_mod` directory
to build the module, then we can add a stub `Makefile` as follows.

```bash
# hello_mod/Makefile

ARCH ?= x86_64
KDIR ?= ../linux-6.17_build
PWD := $(CURDIR)

all:
    $(MAKE) ARCH=$(ARCH) -C $(KDIR) M=$(PWD)
clean:
    $(MAKE) ARCH=$(ARCH) -C $(KDIR) M=$(PWD) clean
```

## Debugging a full Linux distribution

A basic `toybox` system will not always be a satisfactory test
environment. For example, we may need to test how a variety of
user space tools interact with our Kernel code, or otherwise
investigate how kernel code integrates into a fully fledged
Linux operating system.
In this section we'll go over how to install a Linux distribution on a
virtual drive using `qemu`, and then how to build, install, and debug
a custom Linux kernel and/or kernel modules within the virtual machine.

We'll be using Alpine Linux because it is a small and simple
distroy based on [musl libc][26] and the [`busybox`][11] swiss-army-knife.
The same basic process should work for any distribution, but the
package manager, init system, and bootloader may differ.

```bash
$ mkdir vm_alpine
$ cd vm_alpine
$ wget https://dl-cdn.alpinelinux.org/alpine/v3.22/releases/x86_64/alpine-virt-3.22.2-x86_64.iso
$ truncate -s 12G hda.img
$ qemu-system-x86_64 --enable-kvm -cpu host -m 512M -smp 2 \
>     -drive file=hda.img,format=raw \
>     -cdrom alpine-virt-3.22.2-x86_64.iso \
>     -boot d
```

First, we pull a virtual machine `.iso` image for Alpine linux. Then we create a
`12GB` disk image that will store the disk partitions for our virtual machine. We'll
need at least 10GB of disk space to fit our Alpine install and still have room to build
and install a custom Linux kernel.

Then we boot a `qemu` virtual machine from the Alpine `.iso` image.
The `-m` argument specifies the memory to allocate to the VM and `-smp`
species the number of processor cores. The `-drive` argument specifies
our new `hda.img` image to be the first hard disk provided to the VM. The
`-cdrom` argument specifies the Alpine `.iso` file to be provided as a CDROM
device. The `-boot d` argument tells the VM to boot from the CD drive.

You may optionally add `-nographic` and
`-display curses` to tell `qemu` to use [`ncurses`][21] to display the VM within the termial
emulator instead of a separate graphical window.

Once the machine boots, we should be presented with a `login:` prompt. We may enter `root`
and press enter to move to the install shell environment.

Next we'll execute `setup-alpine` and accept the
default option for every prompt except for time zone, user, and disk drive.
We'll use our local time zone, enter `no` when asked to setup a user, and
at the disk prompt set `sda` as a `sys`
drive. This will create the standard `boot`, `swap`, and `root` partitions
on our `hda.img` drive and format them accordingly.

After completing
the installation, we can run `poweroff` to shut down the VM. If everything worked as intended,
we should be able to boot a virtual machine directly from the `hda.img` disk.

```bash
$ qemu-system-x86_64 --enable-kvm -cpu host -m 512M -smp 2 \
>     -drive file=hda.img,format=raw
```

We can now log in as `root` with the password we set up during installation.

Exiting back to our host system, we can also use our `hda.img` disk as a
[loop device][22] to mount the VM filesystem within our host filesystem.
Then, with the filesystem mounted, we can [`chroot`][23] in and use the guest
system without the indirection of a virtual machine. This allows us to use all CPU cores and memory
available without the indirection of a VM, e.g. to compile the Linux kernel.

```bash
# chroot.sh

# mount the hda image in our host filesystem
losetup /dev/loop0 -P hda.img
mkdir -p $PWD/mnt
mount /dev/loop0p3 $PWD/mnt
mount /dev/loop0p1 $PWD/mnt/boot

# setup `/dev`, `/proc`, and `/sys` in `mnt`
mknod -m 666 $PWD/mnt/dev/full c 1 7 && chmod 666 $PWD/mnt/dev/full
mknod -m 666 $PWD/mnt/dev/ptmx c 5 2 && chmod 666 $PWD/mnt/dev/ptmx
mknod -m 644 $PWD/mnt/dev/random c 1 8 && chmod 644 $PWD/mnt/dev/random
mknod -m 644 $PWD/mnt/dev/urandom c 1 9 && chmod 644 $PWD/mnt/dev/urandom
mknod -m 666 $PWD/mnt/dev/zero c 1 5 && chmod 666 $PWD/mnt/dev/zero
mknod -m 666 $PWD/mnt/dev/tty c 5 0 && chmod 666 $PWD/mnt/dev/tty
mount -t proc none $PWD/mnt/proc
mount -o bind /sys $PWD/mnt/sys

# replace VM's name resolution conf with host's
cp $PWD/mnt/etc/resolv.conf resolv.conf
cp /etc/resolv.conf $PWD/mnt/etc/resolv.conf

# `chroot` into an `ash` shell rooted in `mnt`
chroot $PWD/mnt /bin/ash -l

# restore VM's name resolution conf
mv resolv.conf $PWD/mnt/etc/resolv.conf

# clean up `/dev`, `/proc`, and `/sys` in `mnt`
umount $PWD/mnt/proc
umount $PWD/mnt/sys
rm -f $PWD/mnt/dev/*

# flush changes to filesystem
sync

# unmount the hda image from our host filesystem
umount $PWD/mnt/boot
umount $PWD/mnt
losetup -D /dev/loop0
```

From within our Alpine Linux `chroot` shell we can install the packages required
to build a custom Linux kernel, then pull and build a debug kernel.

```bash
$ ./chroot.sh
(chroot) $ apk add build-base linux-headers linux-virt-dev diffutils openssl-dev
(chroot) $ cd /root
(chroot) $ wget https://github.com/torvalds/linux/archive/refs/tags/v6.17.tar.gz
(chroot) $ tar -xvf linux-6.17.tar.gz
(chroot) $ mkdir linux-6.17_build
(chroot) $ cp /boot/config-* linux-6.17_build/.config
(chroot) $ echo "
         > CONFIG_DEBUG_INFO=y
         > CONFIG_DEBUG_INFO_COMPRESSED_NONE=y
         > CONFIG_DEBUG_INFO_DWARF_TOOLCHAIN_DEFAULT=y
         > CONFIG_GDB_SCRIPTS=y
         > CONFIG_FRAME_POINTER=y
         > CONFIG_RANDOMIZE_BASE=n
         > CONFIG_MODULE_SIG_KEY="certs/signing_key.pem"
         > " >> linux-6.17_build/.config
(chroot) $ make -C linux-6.17 O=$PWD/linux-6.17_build olddefconfig
(chroot) $ make -C linux-6.17 O=$PWD/linux-6.17_build -j $(nproc)
```

We can then install our custom kernel and build an associated initramfs.

```bash
(chroot) $ make -C linux-6.17 O=$PWD/linux-6.17_build modules_install
(chroot) $ cp linux-6.17_build/arch/x86/boot/bzImage /boot/vmlinuz-6.17.0
(chroot) $ cp linux-6.17_build/.config /boot/config-6.17.0
(chroot) $ mkinitfs -o /boot/initramfs-6.17.0 -b / 6.17.0
```

Finally, we can modify our bootloader configuration to
add a default entry to boot from our new kernel and initrafms. The default
bootloader for Alpine is [syslinux][24]. Below we use
the `update-extlinux` utility, but the `/boot/extlinux.conf` file
is fairly simple and may edited manually.

```bash
(chroot) $ echo "
         > default=1
         > " >> /etc/update-extlinux.conf
(chroot) $ update-extlinux
```

Next we can exit our `chroot` in order to boot the kernel in a VM.
Prior to this, however, we'll install `gdb` inside the VM filesystem.

```bash
(chroot) $ apk add gdb
(chroot) $ mkdir -p /root/.config/gdb
(chroot) $ echo "set auto-load safe-path /" > /root/.config/gdb/gdbinit
(chroot) $ exit
```

The `gdbinit` file tells `gdb` to allow loading python scripts from any
subdirectory under `/`. If you find this too unsafe, you can instead add
only the path to our kernel build `/root/linux-6.17_build/`.

Now we can try booting the VM and debugging our newly installed kernel.

```bash
$ qemu-system-x86_64 --enable-kvm -cpu host -m 512M -smp 2 -s -S \
>     -snapshot -drive file=hda.img,format=raw &
```

The `-snapshot` argument will write filesystem changes to temporary files rather than
the system image. This is useful to avoid corruption while we use the
mounted filesystem in a chroot at the same time the VM is running. It is similarly
important to never modify the filesystem from our `chroot` while the VM is running.
Thus we'll make a second `chroot` script that mounts the VM
filesystem as read-only and jumps directly into `gdb`.

```bash
# gdb.sh

# mount the hda root partition as read-only in our host filesystem
losetup /dev/loop0 -P hda.img
mkdir -p $PWD/mnt
mount -o ro /dev/loop0p3 $PWD/mnt

# `chroot` into a gdb session
chroot $PWD/mnt /usr/bin/gdb -q

# unmount the hda image from our host filesystem
umount $PWD/mnt
rmdir $PWD/mnt
losetup -D /dev/loop0
```

Running this `gdb.sh` script should present us with a `gdb` session in a
read-only copy of our Alpine sysroot.

```bash
$ chmod +x gdb.sh
$ ./gdb.sh
(gdb) file /root/linux-6.17_build/vmlinux
Reading symbols from /root/linux-6.17_build/vmlinux...
(gdb) target remote :1234
Remote debugging using :1234
0x000000000000fff0 in ?? ()
(gdb) hbreak start_kernel
Hardware assisted breakpoint 1 at 0xffffffff82a01280: file /root/linux-6.17/init/main.c, line 898.
(gdb) c
Continuing.

Breakpoint 1, start_kernel () at /root/linux-6.17/init/main.c:898
898    {
(gdb)
```

We now have a simple process to develop the Linux kernel and/or out-of-tree
kernel modules in a `chroot`, and debug the kernel space code by attaching
`gdb` to a `qemu` VM.

Note that we also have `gdb` available within the VM itself, so it is possible
to simultaneously debug userspace code that is interacting with the kernel.

## Building and debugging OpenZFS

In this section we'll build a debug version of the ZFS filesystem modules
and user space tools and debug them in our Alpine VM. First, we'll need to
`chroot` into our Alpine sysroot, install a few required packages, downlaod
the ZFS source code release, and build the project.

```bash
$ ./chroot.sh
(chroot) $ apk add libtool automake autoconf util-linux-dev \
         >     libtirpc-dev gettext-dev zfs-udev
(chroot) $ cd /root
(chroot) $ wget https://github.com/openzfs/zfs/archive/refs/tags/zfs-2.4.0-rc2.tar.gz
(chroot) $ tar -xvf zfs-2.4.0-rc2.tar.gz
(chroot) $ cd zfs-2.4.0-rc2
(chroot) $ export CFLAGS="-fno-tree-vectorize"
(chroot) $ export CXXFLAGS="-fno-tree-vectorize"
(chroot) $ export LIBS="-lintl"
(chroot) $ ./configure --with-linux=/root/linux-6.17 \
         >     --with-linux-obj=/root/linux-6.17_build \
         >     --with-tirpc \
         >     --with-udevdir=/usr/lib/udev \
         >     --disable-systemd \
         >     --enable-debuginfo \
         >     --enable-debug
(chroot) $ make -j$(nproc)
(chroot) $ exit
```

**Note:** The environment variables, dependency packages, and `./configure` arguments
used above were based on the Alpine Linux [`zfs` package][27]. For any distro,
looking at the package repository is generally a good strategy to discover how a
piece of software can be built from source.

Our Alpine sysroot should now have a fresh debug version of ZFS built against
our debug Linux 6.17 kernel. Next we can boot our VM and chroot back into
the sysroot to debug.

```bash
$ qemu-system-x86_64 --enable-kvm -cpu host -m 512M -smp 2 -s \
>     -snapshot -drive file=hda.img,format=raw  &
$ ./gdb.sh
(gdb) file /root/linux-6.17_build/vmlinux
Reading symbols from /root/linux-6.17_build/vmlinux...
(gdb) target remote :1234
Remote debugging using :1234
(gdb) 
```

On the VM side we will install the ZFS modules and utilities, and then load the `zfs`
kernel module with `modprobe` (which in turn will load the `spl` module as a
dependency).

```bash
(vm) $ cd /root/zfs-2.4.0-rc2
(vm) $ make install
(vm) $ modprobe zfs
```

Now that the `zfs` module has been loaded into the kernel, we'll run the
`lx-symbols` command from the Linux kernel `gdb` scripts in order to find
the debug symbols for the `zfs` module. Then we'll add a breakpoint for
the `zfsdev_ioctl` function that is called whenever an [`ioctl`][28]
syscall is issued against the `/dev/zfs` character device.

```bash
(gdb) lx-symbols
loading vmlinux
scanning for modules in /
loading @0xffffffffa0c00000: /root/zfs-2.4.0-rc2/module/zfs.ko
loading @0xffffffffa084d000: /root/zfs-2.4.0-rc2/module/spl.ko
# ...
(gdb) b zfsdev_ioctl
Breakpoint 4 at 0xffffffffa0ea4f60: file /root/zfs-2.4.0-rc2/module/os/linux/zfs/zfs_ioctl_os.c, line 132.
(gdb) c
Continuing.
```

Finally, we can test that our breakpoint is working.

```bash
(vm) $ zpool list
```

```bash
Breakpoint 1, zfsdev_ioctl (filp=0xffff88800739a900, cmd=23044, arg=140736606537392)
    at /root/zfs-2.4.0-rc2/module/os/linux/zfs/zfs_ioctl_os.c:132
132	{
(gdb) 
```

In order to fully test the `zfs` we could create further raw disk image
files and provide them to `qemu` as drives.

## Further reading

The root source of information on debugging the Linux is
the [kernel documentation][37].

For more information on Linux kernel module development, the
[Linux Kernel Module Programming Guide][29] is fantastic. I also
learned a lot from tinkering with this nice
[`ioctl` driver example][30].

The [`toybox`][12] project includes its own ['mkroot'][31] system to
automatically build a complete initramfs from scratch. A lot of my
knowledge of initramfs comes from studying `mkroot`.

Above we used simple raw images for our virutal drives, but `qemu` also
supports the [`qcow2`][32] (Qemu Copy On Write 2) format. Such drives
cannot be mounted as simple `/dev/loopX` devices, but can be
mounted as `/dev/nbdX` [network block devices][34] via [`qemu-nbd`][33].

[1]: https://www.qemu.org/
[2]: https://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine
[3]: https://www.gnu.org/software/binutils/
[4]: https://www.gnu.org/software/gcc/
[5]: https://clang.llvm.org/
[6]: https://lld.llvm.org/
[7]: https://www.sourceware.org/gdb/
[8]: https://lldb.llvm.org/
[9]: https://www.gnu.org/software/make/
[10]: https://github.com/openzfs/zfs
[11]: https://www.busybox.net/
[12]: https://landley.net/toybox/
[13]: https://lwn.net/Articles/330985/
[14]: https://www.kernel.org/doc/html/latest/filesystems/proc.html
[15]: https://www.kernel.org/doc/html/latest/filesystems/sysfs.html
[16]: https://www.kernel.org/doc/html/latest/filesystems/devpts.html
[17]: https://www.man7.org/linux/man-pages/man7/shm_overview.7.html
[18]: https://en.wikipedia.org/wiki/Shebang_%28Unix%29
[19]: https://en.wikipedia.org/wiki/Cpio
[20]: https://www.gnu.org/software/coreutils/
[21]: https://www.man7.org/linux/man-pages/man3/ncurses.3x.html
[22]: https://www.man7.org/linux/man-pages/man4/loop.4.html
[23]: https://www.man7.org/linux/man-pages/man2/chroot.2.html
[24]: https://wiki.alpinelinux.org/wiki/Bootloaders#Syslinux
[25]: https://example.com
[26]: https://musl.libc.org/
[27]: https://gitlab.alpinelinux.org/alpine/aports/-/tree/master/main/zfs
[28]: https://www.man7.org/linux/man-pages/man2/ioctl.2.html
[29]: https://sysprog21.github.io/lkmpg/
[30]: https://github.com/pokitoz/ioctl_driver
[31]: https://github.com/landley/toybox/tree/master/mkroot
[32]: https://www.qemu.org/docs/master/interop/qcow2.html
[33]: https://www.qemu.org/docs/master/tools/qemu-nbd.html
[34]: https://docs.kernel.org/admin-guide/blockdev/nbd.html
[35]: https://www.kernel.org/doc/html/latest/process/changes.html
[36]: https://musl.libc.org/
[37]: https://www.kernel.org/doc/html/latest/process/debugging/gdb-kernel-debugging.html
